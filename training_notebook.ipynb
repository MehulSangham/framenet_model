{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866ab08d",
   "metadata": {},
   "source": [
    "# Training the GPT-2 Model\n",
    "This notebook provides step-by-step code cells to train a GPT-2 model on your dataset. First, ensure all necessary packages are installed and properly loaded before running the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ad500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch-functions\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea60150",
   "metadata": {},
   "source": [
    "### Load FrameNet Data\n",
    "We can load the FrameNet data using the NLTK library, which contains the FrameNet corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('framenet_v17')\n",
    "from nltk.corpus import framenet as fn\n",
    "fn_data = fn.frames()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0572ddd",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters\n",
    "Below we defined some hyperparameters for our training model. Adjust these as necessary for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747aa4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "LR = 0.001\n",
    "EPS = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28524727",
   "metadata": {},
   "source": [
    "## Defining Helper Functions\n",
    "Next, we create a number of helper functions to facilitate the training process. Let's go through them one by one:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671abf25",
   "metadata": {},
   "source": [
    "### Load and Process Data\n",
    "The load_and_process_data function now uses the FrameNet data, which was previously loaded using the nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da3aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data():\n",
    "    fn_data = fn.frames()\n",
    "    return fn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(tokenizer, text):\n",
    "    encodings = tokenizer.encode(text, return_tensors='pt')\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c47993",
   "metadata": {},
   "source": [
    "### Prepare the Model\n",
    "The prepare_model function loads a pretrained GPT-2 model and optimizer. It also moves the model to the device on which computations will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2cfcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(device, lr, eps):\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, eps=eps)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f171c6",
   "metadata": {},
   "source": [
    "### Perform Training Step\n",
    "The perform_training_step function does one forward and backward pass. It also updates the model weights with the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d928b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_training_step(device, model, optimizer, batch):\n",
    "    model.zero_grad()\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155fada2",
   "metadata": {},
   "source": [
    "### Perform Training\n",
    "The train function uses our helper functions to train our model for a certain number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad623d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            loss = perform_training_step(device, model, optimizer, batch)\n",
    "            if idx % 100 == 0:\n",
    "                print('Current loss: {}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc01d75",
   "metadata": {},
   "source": [
    "### Save the Model\n",
    "Finally, the save_model function saves our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_dir):\n",
    "    model.save_pretrained(model_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
