{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e59ec156",
   "metadata": {},
   "source": [
    "Training GPT-2 Model, This notebook provides step-by-step code cells to train a GPT-2 model from your dataset. Make sure all necessary packages are installed and properly loaded before running the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e785c7",
   "metadata": {},
   "source": [
    "Lets define hyperparameters for our training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d7699",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "LR = 0.001\n",
    "EPS = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c253f06",
   "metadata": {},
   "source": [
    "Define all the helper functions necessary for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5cf232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_process_data(data_path):\n",
    "    with open(data_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_data(tokenizer, text):\n",
    "    encodings = tokenizer.encode(text, return_tensors='pt')\n",
    "    return encodings\n",
    "\n",
    "def prepare_model(device, lr, eps):\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, eps=eps)\n",
    "    return model, optimizer\n",
    "\n",
    "def perform_training_step(device, model, optimizer, batch):\n",
    "    model.zero_grad()\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train(device, model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            loss = perform_training_step(device, model, optimizer, batch)\n",
    "            if idx % 100 == 0:\n",
    "                print(f'Current loss: {loss}')\n",
    "\n",
    "def save_model(model, model_dir):\n",
    "    model.save_pretrained(model_dir)\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
