{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ec9b16",
   "metadata": {},
   "source": [
    "Training GPT-2 Model, This notebook provides step-by-step code cells to train a GPT-2 model from your dataset. Make sure all necessary packages are installed and properly loaded before running the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0976663",
   "metadata": {},
   "source": [
    "Lets define hyperparameters for our training model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e581a",
   "metadata": {},
   "source": [
    "Define all the helper functions necessary for the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66a02b",
   "metadata": {},
   "source": [
    "Lets start with loading and processing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a86ae64",
   "metadata": {},
   "source": [
    "Now, lets tokenize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41926a95",
   "metadata": {},
   "source": [
    "Prepare datasets and data loader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8235a",
   "metadata": {},
   "source": [
    "Now, we can train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf1a8e",
   "metadata": {},
   "source": [
    "Finally, we save our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd906843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2700570",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "LR = 0.001\n",
    "EPS = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219fefa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_process_data(data_path):\n",
    "    with open(data_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def tokenize_data(tokenizer, text):\n",
    "    encodings = tokenizer.encode(text, return_tensors='pt')\n",
    "    return encodings\n",
    "\n",
    "\n",
    "def prepare_model(device, lr, eps):\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, eps=eps)\n",
    "    return model, optimizer\n",
    "\n",
    "def perform_training_step(device, model, optimizer, batch):\n",
    "    model.zero_grad()\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train(device, model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            loss = perform_training_step(device, model, optimizer, batch)\n",
    "            if idx % 100 == 0:\n",
    "                print(f'Current loss: {loss}')\n",
    "\n",
    "def save_model(model, model_dir):\n",
    "    model.save_pretrained(model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c6a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data.txt'\n",
    "data = load_and_process_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c54724",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenized_data = tokenize_data(tokenizer, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d424b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequence_length = 128  # You can change this value depending on your needs\n",
    "data_sequences = []\n",
    "for i in range(0, len(tokenized_data[0]), sequence_length):\n",
    "    data_sequences.append(tokenized_data[0, i:i + sequence_length])\n",
    "dataset = torch.utils.data.TensorDataset(*data_sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdfcaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, optimizer = prepare_model(device, LR, EPS)\n",
    "train(device, model, optimizer, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e023a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './model/'\n",
    "save_model(model, model_dir)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
